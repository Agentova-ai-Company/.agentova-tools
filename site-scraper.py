#!/usr/bin/env python3
"""
Script simple pour tÃ©lÃ©charger toutes les pages d'un site web comme TEXTE PUR

Usage: 
  python site-scraper.py https://example.com [--output-dir ./downloaded-pages] [--max-pages 100] [--delay 1]

FonctionnalitÃ©s:
- Crawl rÃ©cursif de toutes les pages d'un domaine
- Extraction TEXTE PUR (sans HTML, CSS, JS)
- Rate limiting configurable
- Gestion des erreurs simple
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import os
import time
import argparse
import re
from pathlib import Path
from typing import Set, List
import logging

class SiteScraper:
    def __init__(self, base_url: str, output_dir: str = "./downloaded-pages", 
                 max_pages: int = 100, delay: float = 1.0,
                 required_path: str = None, excluded_paths: list = None):
        self.base_url = base_url.rstrip('/')
        self.domain = urlparse(base_url).netloc
        self.output_dir = Path(output_dir)
        self.max_pages = max_pages
        self.delay = delay
        self.required_path = required_path  # Ex: "/docs/v3"
        self.excluded_paths = excluded_paths or []  # Ex: ["/docs/v3/sdks", "/docs/v3/notifications"]
        
        # Ã‰tat du scraping
        self.visited_urls: Set[str] = set()
        self.to_visit: List[str] = [base_url]
        self.failed_urls: List[str] = []
        self.downloaded_count = 0
        
        # Configuration
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Agentova Simple HTML Downloader 1.0'
        })
        
        # Setup logging
        self._setup_logging()
        
        # CrÃ©er dossier output
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def _setup_logging(self):
        """Configure le systÃ¨me de logging simple"""
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
    
    def _is_valid_url(self, url: str) -> bool:
        """VÃ©rifie si l'URL doit Ãªtre tÃ©lÃ©chargÃ©e"""
        parsed = urlparse(url)
        
        # MÃªme domaine uniquement
        if parsed.netloc != self.domain:
            return False
        
        # VÃ©rifier le chemin requis (si spÃ©cifiÃ©)
        if self.required_path and not parsed.path.startswith(self.required_path):
            return False
        
        # VÃ©rifier les chemins exclus
        for excluded_path in self.excluded_paths:
            if parsed.path.startswith(excluded_path):
                return False
        
        # Filtres simples d'extensions
        excluded_extensions = {'.pdf', '.zip', '.exe', '.dmg', '.jpg', '.png', '.gif', '.svg', '.ico', '.css', '.js'}
        if any(url.lower().endswith(ext) for ext in excluded_extensions):
            return False
        
        return True
    
    def _normalize_url(self, url: str) -> str:
        """Normalise une URL (supprime fragments)"""
        parsed = urlparse(url)
        # Supprime le fragment (#)
        normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        # Supprime trailing slash sauf pour root
        if normalized.endswith('/') and len(parsed.path) > 1:
            normalized = normalized[:-1]
        return normalized
    
    def _extract_links(self, soup: BeautifulSoup, current_url: str) -> List[str]:
        """Extrait tous les liens valides d'une page"""
        links = []
        
        for link in soup.find_all('a', href=True):
            href = link['href'].strip()
            if not href or href.startswith('#'):
                continue
            
            # Convertir en URL absolue
            absolute_url = urljoin(current_url, href)
            normalized_url = self._normalize_url(absolute_url)
            
            if self._is_valid_url(normalized_url) and normalized_url not in self.visited_urls:
                links.append(normalized_url)
        
        return list(set(links))  # Supprimer doublons
    
    def _extract_text_content(self, html_content: str) -> str:
        """Extrait uniquement le contenu principal du HTML (sans header/nav/footer)"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Supprimer complÃ¨tement ces Ã©lÃ©ments
        for tag in soup.find_all(['script', 'style', 'noscript', 'iframe', 'embed', 'object']):
            tag.decompose()
        
        # Supprimer les Ã©lÃ©ments de navigation/header/footer mais de faÃ§on plus ciblÃ©e
        for unwanted in soup.select('header, nav, footer'):
            unwanted.decompose()
        
        # Supprimer seulement les Ã©lÃ©ments les plus Ã©vidents de navigation
        unwanted_selectors = [
            '[class*="navigation"]', '[class*="navbar"]', '[class*="sidebar"]', 
            '[id*="navigation"]', '[class*="breadcrumb"]'
        ]
        
        for selector in unwanted_selectors:
            for element in soup.select(selector):
                element.decompose()
        
        # Essayer de trouver le contenu principal de faÃ§on plus permissive
        main_content = (
            soup.find('main') or 
            soup.find('article') or 
            soup.find('div', class_=lambda x: x and 'content' in x.lower()) or
            soup.find('div', id=lambda x: x and 'content' in x.lower()) or
            soup.body or
            soup
        )
        
        # Extraire le texte
        text = main_content.get_text(separator='\n', strip=True)
        
        # Nettoyer le texte (supprimer lignes vides multiples)
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        return '\n'.join(lines)
    
    def _save_html_page(self, url: str, html_content: str):
        """Sauvegarde une page HTML nettoyÃ©e"""
        # CrÃ©er nom de fichier sÃ»r Ã  partir de l'URL
        url_path = urlparse(url).path
        if url_path == '/' or url_path == '':
            filename = 'index'
        else:
            filename = re.sub(r'[^\w\-_.]', '_', url_path.strip('/').replace('/', '_'))
        
        # VÃ©rifier si le fichier existe dÃ©jÃ 
        text_path = self.output_dir / f"{filename}.txt"
        if text_path.exists():
            self.logger.info(f"â­ï¸ Fichier existe dÃ©jÃ : {filename}.txt (ignorÃ©)")
            return filename
        
        # Extraire texte pur
        text_content = self._extract_text_content(html_content)
        
        # Sauvegarder texte pur
        with open(text_path, 'w', encoding='utf-8') as f:
            f.write(text_content)
        
        self.logger.info(f"ğŸ’¾ TÃ©lÃ©chargÃ© (texte pur): {filename}.txt")
        return filename
    
    def download_page(self, url: str) -> bool:
        """TÃ©lÃ©charge une page HTML"""
        try:
            # VÃ©rifier si fichier existe dÃ©jÃ  avant tÃ©lÃ©chargement
            url_path = urlparse(url).path
            if url_path == '/' or url_path == '':
                filename = 'index'
            else:
                filename = re.sub(r'[^\w\-_.]', '_', url_path.strip('/').replace('/', '_'))
            
            text_path = self.output_dir / f"{filename}.txt"
            if text_path.exists():
                self.logger.info(f"â­ï¸ URL dÃ©jÃ  tÃ©lÃ©chargÃ©e: {url} (ignorÃ©)")
                return True
            
            self.logger.info(f"ğŸ“¥ TÃ©lÃ©chargement: {url}")
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            # Sauvegarder HTML
            self._save_html_page(url, response.text)
            self.downloaded_count += 1
            
            # Parser pour extraire liens
            soup = BeautifulSoup(response.content, 'html.parser')
            new_links = self._extract_links(soup, url)
            
            # Ajouter nouveaux liens Ã  la liste
            for link in new_links:
                if link not in self.visited_urls and link not in self.to_visit:
                    self.to_visit.append(link)
            
            self.logger.info(f"âœ… {url} - TrouvÃ© {len(new_links)} nouveaux liens")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur sur {url}: {e}")
            self.failed_urls.append(url)
            return False
    
    def run(self):
        """Lance le tÃ©lÃ©chargement complet"""
        self.logger.info(f"ğŸš€ DÃ©but du tÃ©lÃ©chargement de {self.base_url}")
        self.logger.info(f"ğŸ“ Sortie: {self.output_dir}")
        self.logger.info(f"âš™ï¸ Max pages: {self.max_pages}, DÃ©lai: {self.delay}s")
        
        while self.to_visit and self.downloaded_count < self.max_pages:
            url = self.to_visit.pop(0)
            
            if url in self.visited_urls:
                continue
            
            self.visited_urls.add(url)
            
            if self.download_page(url):
                pass  # Downloaded successfully
            
            # Rate limiting
            if self.delay > 0:
                time.sleep(self.delay)
            
            # Progression
            if self.downloaded_count % 10 == 0 and self.downloaded_count > 0:
                self.logger.info(f"ğŸ“Š Progression: {self.downloaded_count}/{self.max_pages} pages, {len(self.to_visit)} en attente")
        
        # RÃ©sumÃ© final
        self.logger.info(f"ğŸ‰ TÃ©lÃ©chargement terminÃ©!")
        self.logger.info(f"âœ… Pages tÃ©lÃ©chargÃ©es: {self.downloaded_count}")
        self.logger.info(f"âŒ Pages Ã©chouÃ©es: {len(self.failed_urls)}")
        self.logger.info(f"ğŸ“ Fichiers HTML dans: {self.output_dir}")
        
        if self.failed_urls:
            self.logger.info(f"âŒ URLs Ã©chouÃ©es: {', '.join(self.failed_urls[:5])}{'...' if len(self.failed_urls) > 5 else ''}")

def main():
    parser = argparse.ArgumentParser(description='TÃ©lÃ©chargeur simple de pages HTML')
    parser.add_argument('url', help='URL de base du site Ã  tÃ©lÃ©charger')
    parser.add_argument('--output-dir', '-o', default='./downloaded-pages', 
                       help='Dossier de sortie (dÃ©faut: ./downloaded-pages)')
    parser.add_argument('--max-pages', '-m', type=int, default=100,
                       help='Nombre maximum de pages Ã  tÃ©lÃ©charger (dÃ©faut: 100)')
    parser.add_argument('--delay', '-d', type=float, default=1.0,
                       help='DÃ©lai entre requÃªtes en secondes (dÃ©faut: 1.0)')
    parser.add_argument('--required-path', '-r', 
                       help='Chemin requis (ex: /docs/v3)')
    parser.add_argument('--exclude-paths', '-e', nargs='*', default=[],
                       help='Chemins Ã  exclure (ex: /docs/v3/sdks /docs/v3/notifications)')
    
    args = parser.parse_args()
    
    # Validation URL
    if not args.url.startswith(('http://', 'https://')):
        args.url = 'https://' + args.url
    
    print(f"ğŸ”§ Configuration:")
    print(f"   URL: {args.url}")
    print(f"   Sortie: {args.output_dir}")
    print(f"   Max pages: {args.max_pages}")
    print(f"   DÃ©lai: {args.delay}s")
    if args.required_path:
        print(f"   Chemin requis: {args.required_path}")
    if args.exclude_paths:
        print(f"   Chemins exclus: {', '.join(args.exclude_paths)}")
    print()
    
    # Lancer tÃ©lÃ©chargeur
    scraper = SiteScraper(
        base_url=args.url,
        output_dir=args.output_dir,
        max_pages=args.max_pages,
        delay=args.delay,
        required_path=args.required_path,
        excluded_paths=args.exclude_paths
    )
    
    scraper.run()

if __name__ == '__main__':
    main()
